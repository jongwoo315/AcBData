``` r
# 서포트 벡터 머신 (SVM)
install.packages('kernlab', repos="http://cran.nexr.com/")
```

    ## Installing package into '/home/jw/R/x86_64-pc-linux-gnu-library/3.3'
    ## (as 'lib' is unspecified)

``` r
library(kernlab)
(iris.ksvm <- ksvm(Species ~ ., data = iris))  # 기본 커널 함수: gaussian함수
```

    ## Support Vector Machine object of class "ksvm" 
    ## 
    ## SV type: C-svc  (classification) 
    ##  parameter : cost C = 1 
    ## 
    ## Gaussian Radial Basis kernel function. 
    ##  Hyperparameter : sigma =  0.832998805095512 
    ## 
    ## Number of Support Vectors : 59 
    ## 
    ## Objective Function Value : -4.6223 -5.1621 -20.2105 
    ## Training error : 0.026667

``` r
head(predict(iris.ksvm, newdata = iris))
```

    ## [1] setosa setosa setosa setosa setosa setosa
    ## Levels: setosa versicolor virginica

``` r
# 커널함수 바꾸기
(iris.ksvm2 <- ksvm(Species ~ ., data = iris, kernel = 'vanilladot'))
```

    ##  Setting default kernel parameters

    ## Support Vector Machine object of class "ksvm" 
    ## 
    ## SV type: C-svc  (classification) 
    ##  parameter : cost C = 1 
    ## 
    ## Linear (vanilla) kernel function. 
    ## 
    ## Number of Support Vectors : 29 
    ## 
    ## Objective Function Value : -0.9818 -0.322 -17.0644 
    ## Training error : 0.033333

``` r
head(predict(iris.ksvm2, newdata = iris))
```

    ## [1] setosa setosa setosa setosa setosa setosa
    ## Levels: setosa versicolor virginica

``` r
# 커널함수 바꾸기
(iris.ksvm2 <- ksvm(Species ~ ., data = iris, kernel = 'polydot', kpar = list(degree = 3)))
```

    ## Support Vector Machine object of class "ksvm" 
    ## 
    ## SV type: C-svc  (classification) 
    ##  parameter : cost C = 1 
    ## 
    ## Polynomial kernel function. 
    ##  Hyperparameters : degree =  3  scale =  1  offset =  1 
    ## 
    ## Number of Support Vectors : 22 
    ## 
    ## Objective Function Value : -0.0252 -0.0225 -6.3396 
    ## Training error : 0.013333

``` r
head(predict(iris.ksvm2, newdata = iris))
```

    ## [1] setosa setosa setosa setosa setosa setosa
    ## Levels: setosa versicolor virginica

<http://web.mit.edu/~r/current/arch/i386_linux26/lib/R/library/e1071/doc/svmdoc.R>

``` r
# SVM vs RPART
install.packages('mlbench', repos="http://cran.nexr.com/")
```

    ## Installing package into '/home/jw/R/x86_64-pc-linux-gnu-library/3.3'
    ## (as 'lib' is unspecified)

``` r
### R code from vignette source 'svmdoc.Rnw'

###################################################
### code chunk number 1: svmdoc.Rnw:140-149
###################################################
library(e1071)
library(rpart)
data(Glass, package="mlbench")

## split data into a train and test set
index     <- 1:nrow(Glass)
testindex <- sample(index, trunc(length(index)/3))
testset   <- Glass[testindex,]
trainset  <- Glass[-testindex,]

###################################################
### code chunk number 2: svmdoc.Rnw:154-157
###################################################
## svm
svm.model <- svm(Type ~ ., data = trainset, cost = 100, gamma = 1)
svm.pred  <- predict(svm.model, testset[,-10])

###################################################
### code chunk number 3: svmdoc.Rnw:162-165
###################################################
## rpart
rpart.model <- rpart(Type ~ ., data = trainset)
rpart.pred  <- predict(rpart.model, testset[,-10], type = "class")

###################################################
### code chunk number 4: svmdoc.Rnw:168-173
###################################################
## compute svm confusion matrix
table(pred = svm.pred, true = testset[,10])
```

    ##     true
    ## pred  1  2  3  5  6  7
    ##    1 16  3  2  0  0  0
    ##    2  4 23  2  1  4  3
    ##    3  0  1  1  0  0  0
    ##    5  0  0  0  2  0  0
    ##    6  0  0  0  0  0  0
    ##    7  0  0  0  0  0  9

``` r
## compute rpart confusion matrix 
table(pred = rpart.pred, true = testset[,10])
```

    ##     true
    ## pred  1  2  3  5  6  7
    ##    1 15  3  2  0  0  0
    ##    2  5 20  3  0  2  1
    ##    3  0  2  0  0  0  0
    ##    5  0  2  0  3  2  1
    ##    6  0  0  0  0  0  0
    ##    7  0  0  0  0  0 10
