{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "원래 ipynb 제목\n",
    "4. BeautifulSoup-네이버뉴스.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20170308'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 날짜를 이용해 크롤링하는 방법\n",
    "date = time.strftime('%Y%m%d', time.localtime(time.time()))\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 데이터 # 안됨\n",
    "def naverCrawlNews(date=time.strftime('%Y%m%d', time.localtime(time.time())), page_number=2):\n",
    "    with open('naver뉴스_1.txt', 'w') as f:\n",
    "        f.write(date+'일자 속보\\n\\n'.decode('latin1').encode('utf-8'))\n",
    "        \n",
    "    #pre_url = 'http://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001%date=' + str(date) + '&page='\n",
    "    pre_url = 'http://news.naver.com/main/list.nhn?sid1=001&mid=sec&mode=LSD&date=' + str(date) + '&page='\n",
    "    #pre_url = 'http://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001' + '&page='\n",
    "    for page_no in range(1, page_number+1):\n",
    "        pre_url2 = pre_url + str(page_no)\n",
    "        try:\n",
    "            url = urlopen(pre_url2)\n",
    "        except:\n",
    "            print 'urlopen error!'\n",
    "            return None\n",
    "        soup = BeautifulSoup(url, 'lxml')\n",
    "        [x.decompose() for x in soup('photo')]\n",
    "        newslist = soup.find_all(class_='nclicks(fls.list)')\n",
    "        print '%d 페이지 크롤링...' % page_no\n",
    "        for i in newslist:\n",
    "            if(i.string!=None):\n",
    "                with open('naver뉴스.txt', 'a', encoding='utf8') as f:\n",
    "                    f.write(i.text.lstrip() + '\\n' + i[href] + '\\n\\n')\n",
    "        print 'complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 안됨\n",
    "naverCrawlNews('20170308')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따로 실행해서 성공한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(urllib2.urlopen('http://news.naver.com/main/list.nhn?sid1=001&mid=sec&mode=LSD&date=20170308&page=2'), 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 파일로 저장하는 방법\n",
    "text_file = open('result.txt', 'w')\n",
    "text_file.write('asdf %s' % soup)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성공한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 데이터\n",
    "def naverCrawlNews(date=time.strftime('%Y%m%d', time.localtime(time.time())), page_number=2):\n",
    "    with open('naver뉴스.txt', 'w') as f:\n",
    "        f.write(date+' breaking news\\n\\n'.decode('latin1').encode('utf-8'))\n",
    "\n",
    "    # 네이버 뉴스 - 속보 탭\n",
    "    pre_url = 'http://news.naver.com/main/list.nhn?sid1=001&mid=sec&mode=LSD&date=' + str(date) + '&page='\n",
    "    \n",
    "    for page_no in range(1, page_number+1):\n",
    "        pre_url2 = pre_url + str(page_no)\n",
    "        try:\n",
    "            url = urllib2.urlopen(pre_url2)\n",
    "        except:\n",
    "            print 'urlopen error!'\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(url, 'lxml')\n",
    "        \n",
    "        # soup에 들어있는 photo태그를 지운다\n",
    "        [x.decompose() for x in soup('photo')]\n",
    "        newslist = soup.find_all(class_='nclicks(fls.list)')\n",
    "        \n",
    "        print '%d 페이지 크롤링...' % page_no\n",
    "        \n",
    "        for i in newslist:\n",
    "            # 문자열이 비어있지 않다면 naver뉴스.txt에 저장해라\n",
    "            if(i.string!=None):\n",
    "                with open('naver뉴스.txt', 'a') as f:\n",
    "                    f.write(i.text.lstrip().encode('utf8') + '\\n')\n",
    "        print 'complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 페이지 크롤링...\n",
      "complete!\n",
      "2 페이지 크롤링...\n",
      "complete!\n"
     ]
    }
   ],
   "source": [
    "naverCrawlNews()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup - decompose()\n",
    ">removes a tag from the tree, then completely destroys it and its contents:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\n",
    "soup = BeautifulSoup(markup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://example.com/\">I linked to <i>example.com</i></a>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tag = soup.a\n",
    "a_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.i.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://example.com/\">I linked to </a>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
