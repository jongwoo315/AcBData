---
layout: post
title: Python - 17/03/08 (4)
category: acorn수업
---

# 크롤링

---

원래 ipynb 제목  
4. BeautifulSoup-네이버뉴스.ipynb  

<br>

```python
# 날짜를 이용해 크롤링하는 방법
date = time.strftime('%Y%m%d', time.localtime(time.time()))
date
```




    '20170308'


<br>

```python
# 원래 데이터 # 안됨
def naverCrawlNews(date=time.strftime('%Y%m%d', time.localtime(time.time())), page_number=2):
    with open('naver뉴스_1.txt', 'w') as f:
        f.write(date+'일자 속보\n\n'.decode('latin1').encode('utf-8'))

    #pre_url = 'http://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001%date=' + str(date) + '&page='
    pre_url = 'http://news.naver.com/main/list.nhn?sid1=001&mid=sec&mode=LSD&date=' + str(date) + '&page='
    #pre_url = 'http://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001' + '&page='
    for page_no in range(1, page_number+1):
        pre_url2 = pre_url + str(page_no)
        try:
            url = urlopen(pre_url2)
        except:
            print 'urlopen error!'
            return None
        soup = BeautifulSoup(url, 'lxml')
        [x.decompose() for x in soup('photo')]
        newslist = soup.find_all(class_='nclicks(fls.list)')
        print '%d 페이지 크롤링...' % page_no
        for i in newslist:
            if(i.string!=None):
                with open('naver뉴스.txt', 'a', encoding='utf8') as f:
                    f.write(i.text.lstrip() + '\n' + i[href] + '\n\n')
        print 'complete!'
```

<br>

```python
# 안됨
naverCrawlNews('20170308')
```

따로 실행해서 성공한 것

<br>

```python
soup = BeautifulSoup(urllib2.urlopen('http://news.naver.com/main/list.nhn?sid1=001&mid=sec&mode=LSD&date=20170308&page=2'), 'lxml')
```

<br>

```python
soup
```

<br>

```python
# 텍스트 파일로 저장하는 방법
text_file = open('result.txt', 'w')
text_file.write('asdf %s' % soup)
text_file.close()
```

<br>

## 성공한 것


```python
import time
from bs4 import BeautifulSoup
import urllib2
```

<br>

```python
# 원래 데이터
def naverCrawlNews(date=time.strftime('%Y%m%d', time.localtime(time.time())), page_number=2):
    with open('naver뉴스.txt', 'w') as f:
        f.write(date+' breaking news\n\n'.decode('latin1').encode('utf-8'))

    # 네이버 뉴스 - 속보 탭
    pre_url = 'http://news.naver.com/main/list.nhn?sid1=001&mid=sec&mode=LSD&date=' + str(date) + '&page='

    for page_no in range(1, page_number+1):
        pre_url2 = pre_url + str(page_no)
        try:
            url = urllib2.urlopen(pre_url2)
        except:
            print 'urlopen error!'
            return None

        soup = BeautifulSoup(url, 'lxml')

        # soup에 들어있는 photo태그를 지운다
        [x.decompose() for x in soup('photo')]
        newslist = soup.find_all(class_='nclicks(fls.list)')

        print '%d 페이지 크롤링...' % page_no

        for i in newslist:
            # 문자열이 비어있지 않다면 naver뉴스.txt에 저장해라
            if(i.string!=None):
                with open('naver뉴스.txt', 'a') as f:
                    f.write(i.text.lstrip().encode('utf8') + '\n')
        print 'complete!'
```

<br>

```python
naverCrawlNews()
```

    1 페이지 크롤링...
    complete!
    2 페이지 크롤링...
    complete!

<br>

## BeautifulSoup - decompose()
>removes a tag from the tree, then completely destroys it and its contents:  


```python
markup = '<a href="http://example.com/">I linked to <i>example.com</i></a>'
soup = BeautifulSoup(markup)
```

<br>

```python
a_tag = soup.a
a_tag
```




    <a href="http://example.com/">I linked to <i>example.com</i></a>


<br>

```python
soup.i.decompose()
```

<br>

```python
a_tag
```




    <a href="http://example.com/">I linked to </a>
