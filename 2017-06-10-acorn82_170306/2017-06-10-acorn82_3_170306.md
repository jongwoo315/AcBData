---
layout: post
title: Python - 17/03/06 (3)
category: acorn수업
---

# 회귀분석

---

원래 ipynb 제목
3. Chapter 10 - Predicting Continuous Target Variables with Regression Analysis

## <font color='crimson'>Introducing a simple linear regression model

explanatory variable x  
target variable y (continuous valued response)  
w_0는 y의 intercept, w_1은 계수  

$$\hat y = w_0 + w_1x$$

![aa]({{ site.baseurl }}/images/python_acorn/2017-06-10-acorn82_3_170306_files/aa.png)

<br>

## <font color='crimson'>Exploring the Housing Dataset

[
Housing Data Set ](https://archive.ics.uci.edu/ml/datasets/Housing){:target="_blank"}  

보스턴 외곽에 위치한 506개의 집에 대한 정보  

The features of the 506 samples may be summarized as shown in the excerpt of the
dataset description:
- CRIM: This is the per capita crime rate by town (해당 동네의 1인당 범죄비율)
- ZN: This is the proportion of residential land zoned for lots larger than
25,000 sq.ft.
- INDUS: This is the proportion of non-retail business acres per town
- CHAS: This is the Charles River dummy variable (this is equal to 1 if tract
bounds river; 0 otherwise)
- NOX: This is the nitric oxides concentration (parts per 10 million)
- RM: This is the average number of rooms per dwelling (주거지당 평균 방의 개수)
- AGE: This is the proportion of owner-occupied units built prior to 1940
- DIS: This is the weighted distances to five Boston employment centers
- RAD: This is the index of accessibility to radial highways (높을 수록 접근성이 좋음)
- TAX: This is the full-value property-tax rate per \$10,000
- PTRATIO: This is the pupil-teacher ratio by town (해당 동네의 선생 1명당 학생 수의 비율)
- B: This is calculated as 1000(Bk - 0.63)^2, where Bk is the proportion of
people of African American descent by town
- LSTAT: This is the percentage lower status of the population (수치가 높을수록 낮은 지위)
- MEDV: This is the median value of owner-occupied homes in $1000s
for the rest of this chapter, we will regard the housing prices (MEDV) as our
target variable—the variable that we want to predict using one or more of the 13
explanatory variables.

<br>

```python
import pandas as pd
```

<br>

```python
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data', header=None, sep='\s+')
```

<br>

```python
df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',
             'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
```

<br>

```python
df.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
      <td>296</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div>

<br>

#### <font color='coral'>Visualizing the important characteristics of a dataset


```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
```

<br>

```python
# http://seaborn.pydata.org/generated/seaborn.pairplot.html
# pairplot 옵션들
sns.set(style='whitegrid', context='notebook')
cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV']
sns.pairplot(df[cols], size=2.5, markers='*')
plt.show()
```


![png]({{ site.baseurl }}/images/python_acorn/2017-06-10-acorn82_3_170306_files/2017-06-10-acorn82_3_170306_15_0.png)

<br>

```python
import numpy as np
```

<br>

```python
df[cols].T
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>496</th>
      <th>497</th>
      <th>498</th>
      <th>499</th>
      <th>500</th>
      <th>501</th>
      <th>502</th>
      <th>503</th>
      <th>504</th>
      <th>505</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>LSTAT</th>
      <td>4.980</td>
      <td>9.140</td>
      <td>4.030</td>
      <td>2.940</td>
      <td>5.330</td>
      <td>5.210</td>
      <td>12.430</td>
      <td>19.150</td>
      <td>29.930</td>
      <td>17.100</td>
      <td>...</td>
      <td>21.140</td>
      <td>14.100</td>
      <td>12.920</td>
      <td>15.100</td>
      <td>14.330</td>
      <td>9.670</td>
      <td>9.080</td>
      <td>5.640</td>
      <td>6.480</td>
      <td>7.880</td>
    </tr>
    <tr>
      <th>INDUS</th>
      <td>2.310</td>
      <td>7.070</td>
      <td>7.070</td>
      <td>2.180</td>
      <td>2.180</td>
      <td>2.180</td>
      <td>7.870</td>
      <td>7.870</td>
      <td>7.870</td>
      <td>7.870</td>
      <td>...</td>
      <td>9.690</td>
      <td>9.690</td>
      <td>9.690</td>
      <td>9.690</td>
      <td>9.690</td>
      <td>11.930</td>
      <td>11.930</td>
      <td>11.930</td>
      <td>11.930</td>
      <td>11.930</td>
    </tr>
    <tr>
      <th>NOX</th>
      <td>0.538</td>
      <td>0.469</td>
      <td>0.469</td>
      <td>0.458</td>
      <td>0.458</td>
      <td>0.458</td>
      <td>0.524</td>
      <td>0.524</td>
      <td>0.524</td>
      <td>0.524</td>
      <td>...</td>
      <td>0.585</td>
      <td>0.585</td>
      <td>0.585</td>
      <td>0.585</td>
      <td>0.585</td>
      <td>0.573</td>
      <td>0.573</td>
      <td>0.573</td>
      <td>0.573</td>
      <td>0.573</td>
    </tr>
    <tr>
      <th>RM</th>
      <td>6.575</td>
      <td>6.421</td>
      <td>7.185</td>
      <td>6.998</td>
      <td>7.147</td>
      <td>6.430</td>
      <td>6.012</td>
      <td>6.172</td>
      <td>5.631</td>
      <td>6.004</td>
      <td>...</td>
      <td>5.390</td>
      <td>5.794</td>
      <td>6.019</td>
      <td>5.569</td>
      <td>6.027</td>
      <td>6.593</td>
      <td>6.120</td>
      <td>6.976</td>
      <td>6.794</td>
      <td>6.030</td>
    </tr>
    <tr>
      <th>MEDV</th>
      <td>24.000</td>
      <td>21.600</td>
      <td>34.700</td>
      <td>33.400</td>
      <td>36.200</td>
      <td>28.700</td>
      <td>22.900</td>
      <td>27.100</td>
      <td>16.500</td>
      <td>18.900</td>
      <td>...</td>
      <td>19.700</td>
      <td>18.300</td>
      <td>21.200</td>
      <td>17.500</td>
      <td>16.800</td>
      <td>22.400</td>
      <td>20.600</td>
      <td>23.900</td>
      <td>22.000</td>
      <td>11.900</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 506 columns</p>
</div>


<br>

```python
# transpose시키지 않으면 결과가 이상함. corrcoef를 쓰려면 Transpose를 하고 해야 한다!! 위와 같은 형태로!
cm = np.corrcoef(df[cols].T)
cm
```




    array([[ 1.        ,  0.60379972,  0.59087892, -0.61380827, -0.73766273],
           [ 0.60379972,  1.        ,  0.76365145, -0.39167585, -0.48372516],
           [ 0.59087892,  0.76365145,  1.        , -0.30218819, -0.42732077],
           [-0.61380827, -0.39167585, -0.30218819,  1.        ,  0.69535995],
           [-0.73766273, -0.48372516, -0.42732077,  0.69535995,  1.        ]])


<br>

```python
sns.set(font_scale=1.5)
# annot: 블록마다 숫자표시, square:전체가 정사각형형태, annot_kws:숫자의 폰트크기
sns.heatmap(cm, cbar=True, annot=True, square=True,
            fmt='.2f', annot_kws={'size':15},
            yticklabels=cols, xticklabels=cols)
plt.show()
```


![png]({{ site.baseurl }}/images/python_acorn/2017-06-10-acorn82_3_170306_files/2017-06-10-acorn82_3_170306_19_0.png)

<br>

To fit a linear regression model, we are interested in those features that have a high correlation with our target variable MEDV.  

Looking at the preceding correlation matrix, we see that our target variable MEDV shows the largest correlation with the LSTAT variable (-0.74).  
However, as you might remember from the scatterplot matrix, there is a clear nonlinear relationship between LSTAT and MEDV.  

On the other hand, the correlation between RM and MEDV is also relatively high (0.70) and given the linear relationship between those two variables that we observed in the scatterplot,  

RM seems to be a good choice for an exploratory variable to introduce the concepts of a simple linear regression model in the following section.  

<br>

## <font color='crimson'>Implementing an ordinary least squares linear regression model

linear regression can be understood as finding the best-fitting straight line through the sample points of
our training data.  
However, we have neither defined the term best-fitting nor have we discussed the different techniques of fitting such a model.In the following subsections, we will fill in the missing pieces of this puzzle using the
Ordinary Least Squares (OLS) method  
to estimate the parameters of the regression line that
minimizes the sum of the squared vertical distances (residuals or errors) to the sample points.  

<br>

#### <font color='coral'>Solving regression for regression parameters with gradient descent

인공신경망에서 나오는 부분  
- SGD(Stochastic Gradient Descent) - 전진하강법
    - 이상치 제거, 변수 간 종속성을 제거하기 위한 방법
To see our LinearRegressionGD regressor in action,

let's use the RM (number of rooms) variable from the Housing Data Set as the explanatory variable
to train a model that can

predict MEDV (the housing prices).  

Furthermore, we will standardize the variables for better convergence of the GD algorithm.  

<br>

```python
class LinearRegressionGD(object):
    def __init__(self, eta=0.001, n_iter=20):
        self.eta = eta
        self.n_iter = n_iter

    def fit(self, X, y):
        self.w_ = np.zeros(1 + X.shape[1])
        self.cost_ = []

        for i in range(self.n_iter):
            output = self.net_input(X)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            cost = (errors**2).sum() / 2.0
            self.cost_.append(cost)
        return self

    def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def predict(self, X):
        return self.net_input(X)    
```

<br>

```python
X = df[['RM']].values
y = df['MEDV'].values
```

<br>

```python
from sklearn.preprocessing import StandardScaler
```

<br>

```python
sc_x = StandardScaler()
sc_y = StandardScaler()
X_std = sc_x.fit_transform(X)
y_std = sc_y.fit_transform(y)
```

    /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
      warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)
    /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
      warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)


<br>

```python
import numpy as np
print ((X - np.mean(X)) / np.std(X))[:5],'\n'
print ((y - np.mean(y)) / np.std(y))[:5]
```

    [[ 0.41367189]
     [ 0.19427445]
     [ 1.28271368]
     [ 1.01630251]
     [ 1.22857665]]

    [ 0.15968566 -0.10152429  1.32424667  1.18275795  1.48750288]

<br>

```python
# Y값들을 표준화시킨 것 / np.로 한 부분과 동일하다
y_std[:5]
```




    array([ 0.15968566, -0.10152429,  1.32424667,  1.18275795,  1.48750288])


<br>

```python
from sklearn.linear_model import LinearRegression
```

<br>

```python
# X_std는 dataframe, Y_std는 Series
lr = LinearRegressionGD()
lr.fit(X_std, y_std)
```




    <__main__.LinearRegressionGD at 0x7fbd8d2dba50>


plot the cost as a function of the number of epochs (passes over the training dataset)
when we are using optimization algorithms, such as gradient descent,
to check for convergence.

<br>

```python
import matplotlib.pyplot as plt
%matplotlib inline
```

<br>

```python
plt.plot(range(1, lr.n_iter+1), lr.cost_)
plt.ylabel('SSE') # lr.cost_
plt.xlabel('Epoch') # lr.n_iter
plt.show()
```


![png]({{ site.baseurl }}/images/python_acorn/2017-06-10-acorn82_3_170306_files/2017-06-10-acorn82_3_170306_37_0.png)  

GD algorithm converged after the fifth epoch

<br>

#### <font color='lime'>visualize the training data to see how well the linear regression line fits


```python
def lin_regplot(X, y, model):
    plt.scatter(X, y, c='royalblue')
    plt.plot(X, model.predict(X), color='red')
    return None
```

<br>

```python
lin_regplot(X_std, y_std, lr)
plt.xlabel('Average number of rooms [RM] (standardized)')
plt.ylabel('Price in $1000\'s [MEDV] (standardized)')
plt.show()
```


![png]({{ site.baseurl }}/images/python_acorn/2017-06-10-acorn82_3_170306_files/2017-06-10-acorn82_3_170306_41_0.png)

<br>

#### <font color='lime'>predict house price where room size is 5

the data tells us that the number of rooms does not explain the house prices very well in many cases.  

Interestingly, we also observe a curious line y = 3 , which suggests that the prices may have been clipped.  


it may also be important to report the predicted outcome variables on its original scale.  

To scale the predicted price outcome back on the Price in \$1000's axes,
we can simply apply the inverse_transform method of the StandardScaler :we used the previously trained linear regression model to predict the price of a house with five rooms. According to our model, such a house is worth \$10,840.  

```python
# transform과 fit_transform은 같은 함수 (표준화시킴)
# 방이 5개인 집의 가격

num_rooms_std = sc_x.transform([5.0]) # (5 - sc_x.mean_) / sc_x.std_ 이값임
# sc_x에는 X축(RM 컬럼)의 값들이 들어가 있다
price_std = lr.predict(num_rooms_std)
# 표준화된 집가격을 다시 원래 scale의 값으로 바꾼다
print 'price in $1000\'s', sc_y.inverse_transform(price_std)
```

    price in $1000's 10.8399328886


    /usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
      warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)

<br>

if we are working with standardized variables, axis intercept is always 0  

```python
print '%.3f' % lr.w_[1] # 계수(기울기)
print '%.3f' % lr.w_[0] # 절편(intercept)
```

    0.695
    -0.000

<br>

#### <font color='coral'>Estimating the coefficient of a regression model via scikit-learn

## <font color='crimson'>Fitting a robust regression model using RANSAC

## <font color='crimson'>Evaluating the performance of linear regression models

## <font color='crimson'>Using regularized methods for regression

## <font color='crimson'>Turning a linear regression model into a curve – polynomial regression

#### <font color='coral'>Modeling nonlinear relationships in the Housing Dataset

#### <font color='coral'>Dealing with nonlinear relationships using random forests

##### Decision tree regression

##### Random forest regression

## <font color='crimson'>Summary
